{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.argv += \"-log tb -name tst -reset 1 -state_size 64 -load_pretrained_model ../checkpoints/gru_64_with_input.pth -var_analysis.no_input 0\".split(\" \")\n",
    "\n",
    "# Pretend we are in the main directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from main import initialize\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Note: checkpoints have all arguments saved\n",
    "helper, task = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def plot_states(bi, enc_states, dec_states, data, diff=False):\n",
    "    # plt.figure()\n",
    "\n",
    "\n",
    "    # plt.imshow(all_states[:, bi].T.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "\n",
    "    all_states = torch.cat([enc_states[:data[\"in_len\"][bi]], dec_states[:data[\"out_len\"][bi]]], dim=0)\n",
    "    all_states = all_states.permute(0,2,1,3).flatten(2)\n",
    "\n",
    "    diff_states = (all_states - F.pad(all_states[:-1], (0, 0, 0, 0, 1, 0), value=0))\n",
    "\n",
    "    in_data = data[\"in\"][:data[\"in_len\"][bi], bi]\n",
    "    out_data = data[\"out\"][:data[\"out_len\"][bi], bi]\n",
    "    in_data = task.train_set.in_vocabulary(in_data.cpu().numpy().tolist())\n",
    "    out_data = task.train_set.out_vocabulary(out_data.cpu().numpy().tolist())\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "    plt.axes(axs[0])\n",
    "    plt.imshow(all_states[:, bi].T.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "    plt.xticks(range(len(in_data) + len(out_data)), in_data + [\"S\"] + out_data[:-1])\n",
    "\n",
    "    plt.axes(axs[1])\n",
    "    plt.imshow(diff_states[:, bi].T.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "    plt.xticks(range(len(in_data) + len(out_data)), in_data + [\"S\"] + out_data[:-1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def encode_with_state(self, inp: torch.Tensor, in_len: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.embedding(inp.long())\n",
    "\n",
    "    state = None\n",
    "    states = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        _, new_state = self.rnn(x[i:i+1], state)\n",
    "        state = torch.where((i < in_len).view(1, -1, 1), new_state, state) if state is not None else new_state\n",
    "        states.append(state)\n",
    "\n",
    "    states = torch.stack(states, dim=0)\n",
    "    return states[-1], states\n",
    "\n",
    "def decode_with_state(self, encoded_state: torch.Tensor, outp: torch.Tensor, out_len: torch.Tensor) -> torch.Tensor:\n",
    "    if self.no_input:\n",
    "        outp = torch.full_like(outp, self.no_input_token)\n",
    "\n",
    "    x = F.pad(outp[:-1], (0, 0, 1, 0), value=self.sos_token)\n",
    "    x = self.embedding(x.long())\n",
    "\n",
    "    out_seq = []\n",
    "    state = encoded_state\n",
    "    states = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        out, state = self.rnn(x[i:i+1], state)\n",
    "        states.append(state)\n",
    "        out_seq.append(out)\n",
    "\n",
    "    out = torch.cat(out_seq, dim=0)\n",
    "    states = torch.stack(states, dim=0)\n",
    "    return self.fc(out), states\n",
    "\n",
    "task.model.encode_with_state = encode_with_state.__get__(task.model)\n",
    "task.model.decode_with_state = decode_with_state.__get__(task.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def create_input(input: str) -> torch.Tensor:\n",
    "    a = task.train_set.in_vocabulary(input)\n",
    "    inp = torch.tensor(a, device=helper.device).unsqueeze(1)\n",
    "    in_len = torch.tensor([len(a)], device=helper.device)\n",
    "\n",
    "    return inp, in_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def run_model(input: str):\n",
    "    inp, in_len = create_input(input)\n",
    "\n",
    "    task.set_eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_state, states = task.model.encode_with_state(inp, in_len)\n",
    "        out, dec_states = task.model.decode_with_state(encoded_state, inp, in_len)\n",
    "\n",
    "        out = out.argmax(dim=-1).squeeze(1)\n",
    "        print(task.train_set.out_vocabulary(out.cpu().numpy()))\n",
    "        print((out==inp.squeeze(1)).int().cpu().numpy())\n",
    "\n",
    "        plot_states(0, states, dec_states, {\"in\": inp, \"in_len\": in_len, \"out\": inp, \"out_len\": in_len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_model(\"b b b b c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_model(\"b a a b b b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def plot_diff(i1: str, i2: str):\n",
    "    inputs = [create_input(i1), create_input(i2)]\n",
    "    states = []\n",
    "\n",
    "    task.set_eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, in_len) in enumerate(inputs):\n",
    "            encoded_state, enc_states = task.model.encode_with_state(inp, in_len)\n",
    "            out, dec_states = task.model.decode_with_state(encoded_state, inp, in_len)\n",
    "\n",
    "            all_states = torch.cat([enc_states, dec_states], dim=0).squeeze(-2)\n",
    "            all_states = all_states.permute(0,2,1).flatten(1)\n",
    "            states.append(all_states)\n",
    "\n",
    "            out = out.argmax(dim=-1).squeeze(1)\n",
    "            print(task.train_set.out_vocabulary(out.cpu().numpy()))\n",
    "            print((out==inp.squeeze(1)).cpu().numpy())\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow((states[0]-states[1]).T.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\", vmin=-1, vmax=1)\n",
    "\n",
    "    labels = [f\"{a}/{b}\" for a, b in zip(i1.split(),i2.split())]\n",
    "    xtok = labels + [\"S\"] + labels[:-1]\n",
    "    plt.xticks(range(len(xtok)), xtok)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_diff(\"a a a a a\", \"a a b a a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_diff(\"a a a c a\", \"a a b a a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_model(\"a a a a a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_model(\"b b b b b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_model(\"a b c d c b a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_diff(\"a b c d a a a a\", \"c a d b a a a a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
